{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VedantKothari01/DocInsight/blob/main/DocInsight_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 DocInsight Enhanced - Complete Real Dataset Demo\n",
    "\n",
    "## 🚀 Production-Ready Plagiarism Detection with Real Datasets\n",
    "\n",
    "**What's New in v2.0:**\n",
    "- ✅ **Real Dataset Integration**: PAWS, Wikipedia, arXiv (50,000+ sentences)\n",
    "- ✅ **NO Hardcoded Corpus**: Purely data-driven approach\n",
    "- ✅ **Complete Document Analysis**: Every sentence analyzed, not just highlights\n",
    "- ✅ **Advanced ML Pipeline**: SentenceTransformers + Cross-encoder + Stylometry\n",
    "- ✅ **Production-Ready Web Interface**: One-click document upload and analysis\n",
    "\n",
    "This notebook demonstrates the complete DocInsight system from setup to analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Installation and Setup\n",
    "\n",
    "Install all required dependencies for real dataset integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install -q sentence-transformers faiss-cpu transformers datasets wikipedia\n",
    "!pip install -q spacy textstat python-docx pymupdf streamlit requests\n",
    "\n",
    "# Download spaCy model (optional, enhances linguistic analysis)\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "print(\"✅ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 Real Dataset Integration\n",
    "\n",
    "Load and test the real dataset integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"🔍 DocInsight Enhanced v2.0\")\n",
    "print(\"Real Dataset Integration Demo\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset loading capabilities\n",
    "from dataset_loaders import DatasetLoader\n",
    "\n",
    "# Initialize dataset loader\n",
    "loader = DatasetLoader()\n",
    "\n",
    "print(\"📊 Testing dataset loading capabilities...\")\n",
    "print()\n",
    "\n",
    "# Test PAWS dataset loading\n",
    "print(\"1. 🔤 Loading PAWS paraphrase dataset...\")\n",
    "start_time = time.time()\n",
    "paws_sentences = loader.load_paws_dataset(max_samples=1000)\n",
    "paws_time = time.time() - start_time\n",
    "print(f\"   ✅ Loaded {len(paws_sentences)} PAWS sentences in {paws_time:.1f}s\")\n",
    "if paws_sentences:\n",
    "    print(f\"   📝 Sample: {paws_sentences[0][:100]}...\")\n",
    "print()\n",
    "\n",
    "# Test Wikipedia loading\n",
    "print(\"2. 📚 Loading Wikipedia articles...\")\n",
    "start_time = time.time()\n",
    "wiki_topics = [\"Machine learning\", \"Climate change\", \"Healthcare\"]\n",
    "wiki_sentences = loader.load_wikipedia_articles(topics=wiki_topics, sentences_per_topic=50)\n",
    "wiki_time = time.time() - start_time\n",
    "print(f\"   ✅ Loaded {len(wiki_sentences)} Wikipedia sentences in {wiki_time:.1f}s\")\n",
    "if wiki_sentences:\n",
    "    print(f\"   📝 Sample: {wiki_sentences[0][:100]}...\")\n",
    "print()\n",
    "\n",
    "# Test arXiv loading\n",
    "print(\"3. 🎓 Loading arXiv academic papers...\")\n",
    "start_time = time.time()\n",
    "arxiv_categories = [\"cs.AI\", \"cs.CL\"]\n",
    "arxiv_sentences = loader.load_arxiv_abstracts(categories=arxiv_categories, max_papers=100)\n",
    "arxiv_time = time.time() - start_time\n",
    "print(f\"   ✅ Loaded {len(arxiv_sentences)} arXiv sentences in {arxiv_time:.1f}s\")\n",
    "if arxiv_sentences:\n",
    "    print(f\"   📝 Sample: {arxiv_sentences[0][:100]}...\")\n",
    "print()\n",
    "\n",
    "total_sentences = len(paws_sentences) + len(wiki_sentences) + len(arxiv_sentences)\n",
    "print(f\"🎉 Successfully loaded {total_sentences} sentences from real datasets!\")\n",
    "print(f\"📊 Sources: PAWS ({len(paws_sentences)}), Wikipedia ({len(wiki_sentences)}), arXiv ({len(arxiv_sentences)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Corpus Building and ML Pipeline\n",
    "\n",
    "Build the complete corpus with semantic embeddings and search index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus_builder import CorpusIndex\n",
    "from enhanced_pipeline import PlagiarismDetector\n",
    "\n",
    "print(\"🏗️ Building DocInsight corpus and ML pipeline...\")\n",
    "print()\n",
    "\n",
    "# Create corpus index with reasonable size for demo\n",
    "print(\"1. 📚 Building corpus from real datasets...\")\n",
    "start_time = time.time()\n",
    "corpus_index = CorpusIndex(target_size=5000)  # 5K sentences for demo\n",
    "success = corpus_index.load_or_build()\n",
    "corpus_time = time.time() - start_time\n",
    "\n",
    "if not success:\n",
    "    print(\"❌ Failed to build corpus - check network connection\")\n",
    "    raise RuntimeError(\"Cannot proceed without real datasets\")\n",
    "\n",
    "print(f\"   ✅ Built corpus with {len(corpus_index.sentences)} sentences in {corpus_time:.1f}s\")\n",
    "print()\n",
    "\n",
    "# Build semantic embeddings\n",
    "print(\"2. 🧠 Generating semantic embeddings...\")\n",
    "start_time = time.time()\n",
    "corpus_index.build_embeddings()\n",
    "embed_time = time.time() - start_time\n",
    "print(f\"   ✅ Generated embeddings in {embed_time:.1f}s\")\n",
    "print()\n",
    "\n",
    "# Build FAISS search index\n",
    "print(\"3. ⚡ Building FAISS search index...\")\n",
    "start_time = time.time()\n",
    "index_built = corpus_index.build_index()\n",
    "index_time = time.time() - start_time\n",
    "\n",
    "if index_built:\n",
    "    print(f\"   ✅ Built FAISS index in {index_time:.1f}s\")\n",
    "else:\n",
    "    print(f\"   ⚠️ FAISS not available, using fallback search\")\n",
    "print()\n",
    "\n",
    "# Initialize plagiarism detector\n",
    "print(\"4. 🔍 Initializing plagiarism detector...\")\n",
    "detector = PlagiarismDetector(corpus_index)\n",
    "print(\"   ✅ Detector ready with complete ML pipeline\")\n",
    "print()\n",
    "\n",
    "total_time = corpus_time + embed_time + index_time\n",
    "print(f\"🎉 DocInsight fully initialized in {total_time:.1f}s!\")\n",
    "print(f\"📊 Ready for plagiarism detection with {len(corpus_index.sentences)} real sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Plagiarism Detection Demo\n",
    "\n",
    "Test the complete plagiarism detection pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"🔍 DocInsight Plagiarism Detection Demo\")\n",
    "print(\"=\" * 45)\n",
    "print()\n",
    "\n",
    "# Test sentences with varying similarity levels\n",
    "test_sentences = [\n",
    "    \"Machine learning algorithms can identify patterns in large datasets.\",  # Likely in corpus\n",
    "    \"Artificial intelligence systems are becoming increasingly sophisticated.\",  # Common topic\n",
    "    \"Climate change poses significant challenges to global sustainability.\",  # Wikipedia content\n",
    "    \"The quick brown fox jumps over the lazy dog.\",  # Unlikely match\n",
    "    \"Neural networks utilize backpropagation for training deep architectures.\",  # Technical\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing individual sentence analysis:\")\n",
    "print()\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"{i}. Analyzing: '{sentence}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = detector.analyze_sentence(sentence)\n",
    "    analysis_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   📊 Fused Score: {result.fused_score:.3f}\")\n",
    "    print(f\"   🎯 Confidence: {result.confidence}\")\n",
    "    print(f\"   📝 Stylometry Score: {result.stylometry_score:.3f}\")\n",
    "    print(f\"   ⏱️ Analysis Time: {analysis_time:.3f}s\")\n",
    "    \n",
    "    if result.matches:\n",
    "        print(f\"   🔍 Top Match: {result.matches[0].similarity:.3f} - '{result.matches[0].text[:60]}...'\")\n",
    "    else:\n",
    "        print(\"   ℹ️ No significant matches found\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"✅ Individual sentence analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 Complete Document Analysis\n",
    "\n",
    "Test comprehensive document analysis (the main feature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document for testing\n",
    "test_document = \"\"\"\n",
    "Machine learning has become a fundamental tool in modern artificial intelligence. \n",
    "Neural networks can approximate complex non-linear functions through deep architectures. \n",
    "The training process involves optimizing weights using gradient descent algorithms. \n",
    "Natural language processing enables computers to understand and generate human text. \n",
    "Computer vision systems analyze visual information to recognize objects and patterns.\n",
    "\n",
    "Climate change represents one of the most pressing challenges of our time. \n",
    "Greenhouse gas emissions from human activities are warming the planet. \n",
    "Renewable energy sources offer sustainable alternatives to fossil fuels. \n",
    "Biodiversity loss threatens ecosystem stability and resilience worldwide.\n",
    "\n",
    "This document contains a mix of technical and environmental content. \n",
    "Some sentences may have high similarity to existing literature. \n",
    "The plagiarism detection system should identify potential matches. \n",
    "Overall analysis will provide comprehensive risk assessment.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"📄 Complete Document Analysis Demo\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "print(f\"📝 Document Length: {len(test_document)} characters\")\n",
    "print(f\"📊 Estimated Sentences: ~{len(test_document.split('.'))}\")\n",
    "print()\n",
    "\n",
    "print(\"🔍 Analyzing entire document...\")\n",
    "start_time = time.time()\n",
    "doc_results = detector.analyze_document(test_document)\n",
    "analysis_time = time.time() - start_time\n",
    "\n",
    "print(f\"✅ Analysis complete in {analysis_time:.2f}s\")\n",
    "print()\n",
    "\n",
    "# Display overall statistics\n",
    "stats = doc_results['overall_stats']\n",
    "print(\"📊 OVERALL DOCUMENT STATISTICS:\")\n",
    "print(f\"   📝 Total Sentences Analyzed: {stats['total_sentences']}\")\n",
    "print(f\"   📈 Average Similarity Score: {stats['avg_fused_score']:.3f}\")\n",
    "print(f\"   🔝 Maximum Similarity Score: {stats['max_fused_score']:.3f}\")\n",
    "print(f\"   🔴 HIGH Risk Sentences: {stats['high_confidence_count']}\")\n",
    "print(f\"   🟡 MEDIUM Risk Sentences: {stats['medium_confidence_count']}\")\n",
    "print(f\"   🟢 LOW Risk Sentences: {stats['low_confidence_count']}\")\n",
    "print(f\"   ⚠️ High Risk Percentage: {stats['high_risk_ratio']*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Risk assessment\n",
    "if stats['high_confidence_count'] > 0:\n",
    "    risk_level = \"🔴 HIGH RISK\"\n",
    "    recommendation = \"Immediate review required - potential plagiarism detected\"\n",
    "elif stats['medium_confidence_count'] > 0:\n",
    "    risk_level = \"🟡 MEDIUM RISK\"\n",
    "    recommendation = \"Manual review recommended - moderate similarities found\"\n",
    "else:\n",
    "    risk_level = \"🟢 LOW RISK\"\n",
    "    recommendation = \"No significant plagiarism concerns detected\"\n",
    "\n",
    "print(f\"🚨 RISK ASSESSMENT: {risk_level}\")\n",
    "print(f\"💡 RECOMMENDATION: {recommendation}\")\n",
    "print()\n",
    "\n",
    "print(\"📋 DETAILED SENTENCE-BY-SENTENCE ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, analysis in enumerate(doc_results['sentence_analyses'], 1):\n",
    "    confidence = analysis['confidence']\n",
    "    score = analysis['fused_score']\n",
    "    sentence = analysis['sentence']\n",
    "    matches = analysis['matches']\n",
    "    \n",
    "    # Color coding\n",
    "    if confidence == \"HIGH\":\n",
    "        emoji = \"🔴\"\n",
    "    elif confidence == \"MEDIUM\":\n",
    "        emoji = \"🟡\"\n",
    "    else:\n",
    "        emoji = \"🟢\"\n",
    "    \n",
    "    print(f\"{emoji} Sentence {i}: {confidence} RISK (Score: {score:.3f})\")\n",
    "    print(f\"   Text: {sentence}\")\n",
    "    \n",
    "    if matches:\n",
    "        print(f\"   🔍 {len(matches)} similar matches found:\")\n",
    "        for j, match in enumerate(matches[:2], 1):  # Show top 2\n",
    "            print(f\"      {j}. Similarity: {match['similarity']:.3f} - {match['text'][:80]}...\")\n",
    "    else:\n",
    "        print(\"   ℹ️ No significant matches\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"✅ Complete document analysis finished!\")\n",
    "print(f\"📊 Analyzed {stats['total_sentences']} sentences in {analysis_time:.2f}s\")\n",
    "print(f\"⚡ Average time per sentence: {analysis_time/stats['total_sentences']:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Report Generation\n",
    "\n",
    "Generate comprehensive reports in multiple formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"📋 Report Generation Demo\")\n",
    "print(\"=\" * 30)\n",
    "print()\n",
    "\n",
    "# Generate JSON report\n",
    "print(\"1. 📄 Generating JSON report...\")\n",
    "json_report = json.dumps(doc_results, indent=2, ensure_ascii=False)\n",
    "print(f\"   ✅ JSON report generated ({len(json_report)} characters)\")\n",
    "\n",
    "# Save JSON report\n",
    "with open('plagiarism_report.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json_report)\n",
    "print(\"   💾 Saved as 'plagiarism_report.json'\")\n",
    "print()\n",
    "\n",
    "# Generate summary report\n",
    "print(\"2. 📊 Generating summary report...\")\n",
    "summary_report = f\"\"\"DocInsight Enhanced - Plagiarism Analysis Report\n",
    "================================================\n",
    "\n",
    "Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Document Length: {len(test_document)} characters\n",
    "Corpus Source: Real datasets (PAWS, Wikipedia, arXiv)\n",
    "Total Corpus Size: {len(corpus_index.sentences)} sentences\n",
    "\n",
    "OVERALL STATISTICS:\n",
    "- Total Sentences Analyzed: {stats['total_sentences']}\n",
    "- Average Similarity Score: {stats['avg_fused_score']:.3f}\n",
    "- Maximum Similarity Score: {stats['max_fused_score']:.3f}\n",
    "- HIGH Risk Sentences: {stats['high_confidence_count']}\n",
    "- MEDIUM Risk Sentences: {stats['medium_confidence_count']}\n",
    "- LOW Risk Sentences: {stats['low_confidence_count']}\n",
    "- High Risk Percentage: {stats['high_risk_ratio']*100:.1f}%\n",
    "\n",
    "RISK ASSESSMENT:\n",
    "{risk_level} - {recommendation}\n",
    "\n",
    "METHODOLOGY:\n",
    "- Semantic Similarity: SentenceTransformers (all-MiniLM-L6-v2)\n",
    "- Cross-encoder Reranking: ms-marco-MiniLM-L-6-v2\n",
    "- Stylometry Analysis: 15+ linguistic features\n",
    "- Search Index: FAISS for fast similarity search\n",
    "- Score Fusion: Weighted combination of multiple signals\n",
    "\n",
    "DATA SOURCES:\n",
    "- PAWS: Paraphrase Adversaries from Word Scrambling\n",
    "- Wikipedia: Multi-domain encyclopedia articles\n",
    "- arXiv: Academic paper abstracts\n",
    "\n",
    "Generated by DocInsight Enhanced v2.0\n",
    "Real Dataset Integration - No Hardcoded Corpus\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "with open('plagiarism_summary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"   ✅ Summary report generated ({len(summary_report)} characters)\")\n",
    "print(\"   💾 Saved as 'plagiarism_summary.txt'\")\n",
    "print()\n",
    "\n",
    "print(\"📋 Sample of summary report:\")\n",
    "print(\"-\" * 40)\n",
    "print(summary_report[:800] + \"...\")\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"✅ Report generation complete!\")\n",
    "print(\"📁 Files generated: plagiarism_report.json, plagiarism_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 Streamlit Web Interface\n",
    "\n",
    "Launch the production-ready web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 DocInsight Web Interface Setup\")\n",
    "print(\"=\" * 35)\n",
    "print()\n",
    "\n",
    "print(\"📋 Instructions to run the Streamlit web interface:\")\n",
    "print()\n",
    "print(\"1. 📁 Save this notebook and ensure all files are in the same directory:\")\n",
    "print(\"   - corpus_builder.py\")\n",
    "print(\"   - dataset_loaders.py\")\n",
    "print(\"   - enhanced_pipeline.py\")\n",
    "print(\"   - streamlit_app.py\")\n",
    "print(\"   - requirements.txt\")\n",
    "print()\n",
    "print(\"2. 🖥️ Open a terminal/command prompt and navigate to the directory\")\n",
    "print()\n",
    "print(\"3. 🚀 Run the following command:\")\n",
    "print(\"   streamlit run streamlit_app.py\")\n",
    "print()\n",
    "print(\"4. 🌐 The web interface will open at: http://localhost:8501\")\n",
    "print()\n",
    "print(\"🎯 Web Interface Features:\")\n",
    "print(\"- 📤 Upload documents (TXT, PDF, DOCX)\")\n",
    "print(\"- 🔍 One-click plagiarism analysis\")\n",
    "print(\"- 📊 Complete document review with confidence scoring\")\n",
    "print(\"- 📋 Downloadable reports (JSON + summary)\")\n",
    "print(\"- 🌐 Real dataset integration (no hardcoded corpus)\")\n",
    "print(\"- ⚡ FAISS-powered fast search\")\n",
    "print()\n",
    "\n",
    "# Alternative: Run demo script\n",
    "print(\"🎁 Alternative - Complete Demo Script:\")\n",
    "print(\"   python docinsight_demo.py\")\n",
    "print(\"   (Includes automatic setup + Streamlit launch)\")\n",
    "print()\n",
    "\n",
    "print(\"✅ DocInsight Enhanced is ready for production use!\")\n",
    "print(\"🔍 Upload any document and get comprehensive plagiarism analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Summary and Next Steps\n",
    "\n",
    "**DocInsight Enhanced v2.0 Features Demonstrated:**\n",
    "\n",
    "✅ **Real Dataset Integration**: \n",
    "- PAWS paraphrase dataset for advanced paraphrase detection\n",
    "- Wikipedia articles covering 25+ diverse topics\n",
    "- arXiv academic papers for scholarly content\n",
    "- **NO hardcoded fallback corpus**\n",
    "\n",
    "✅ **Complete Document Analysis**:\n",
    "- Every sentence analyzed and scored\n",
    "- Confidence levels: HIGH/MEDIUM/LOW\n",
    "- Comprehensive risk assessment\n",
    "- Detailed similarity matches with sources\n",
    "\n",
    "✅ **Advanced ML Pipeline**:\n",
    "- SentenceTransformers for semantic similarity\n",
    "- Cross-encoder reranking for precision\n",
    "- Stylometry analysis with 15+ linguistic features\n",
    "- FAISS indexing for sub-second search\n",
    "\n",
    "✅ **Production-Ready Interface**:\n",
    "- Streamlit web app with file upload\n",
    "- One-click analysis workflow\n",
    "- Multiple report formats (JSON, summary)\n",
    "- Real-time progress indicators\n",
    "\n",
    "**🎯 Ready for Production Use:**\n",
    "1. Run `python docinsight_demo.py` for complete setup\n",
    "2. Upload any document (TXT, PDF, DOCX) via web interface\n",
    "3. Get comprehensive plagiarism analysis report\n",
    "4. No manual corpus upload needed - fully automated!\n",
    "\n",
    "**🚀 This system now provides enterprise-grade plagiarism detection with real dataset integration!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}