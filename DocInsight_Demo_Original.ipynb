{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VedantKothari01/DocInsight/blob/main/DocInsight_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e6e713",
      "metadata": {
        "id": "20e6e713"
      },
      "source": [
        "\n",
        "## DocInsight : Demo Version 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e27915",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00e27915",
        "outputId": "de4f5a02-5241-416b-dd16-8490181691c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Install complete. If Colab prompts to restart the runtime after spaCy install, please DO restart and then continue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q sentence-transformers faiss-cpu transformers datasets spacy textstat python-docx pymupdf docx2txt nltk streamlit\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea017c20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea017c20",
        "outputId": "89759b5b-0cfe-4f1e-8878-bbbfad8cec80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, json, math, tempfile, html, time\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "import faiss\n",
        "import spacy\n",
        "import textstat\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import docx2txt\n",
        "import fitz  #PyMuPDF\n",
        "\n",
        "print('Imports ready.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12608b7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12608b7e",
        "outputId": "4dbb7a28-3a46-4b45-e7c8-12731fb0f566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing utilities ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Utilities: parsing PDFs and DOCX into text and sentence-splits ---\n",
        "\n",
        "def extract_text_from_pdf(path):\n",
        "    text = []\n",
        "    doc = fitz.open(path)\n",
        "    for page in doc:\n",
        "        text.append(page.get_text())\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def extract_text(path):\n",
        "    path = str(path)\n",
        "    ext = Path(path).suffix.lower()\n",
        "    if ext == '.pdf':\n",
        "        return extract_text_from_pdf(path)\n",
        "    elif ext == '.docx':\n",
        "        return docx2txt.process(path)\n",
        "    elif ext in ['.txt']:\n",
        "        with open(path,'r',encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    else:\n",
        "        raise ValueError('Unsupported file type: ' + ext)\n",
        "\n",
        "\n",
        "def split_sentences(text):\n",
        "\n",
        "    #basic sentence tokenizer using nltk's punkt\n",
        "    sents = sent_tokenize(text)\n",
        "\n",
        "    #strip and filter short sentences\n",
        "    sents = [s.strip() for s in sents if len(s.strip())>3]\n",
        "    return sents\n",
        "print('Parsing utilities ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00b45180",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00b45180",
        "outputId": "729dbffa-ff0b-4df7-b1a4-76c5acc0f758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demo files created in /mnt/data/demo\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Demo corpus and sample documents ---\n",
        "\n",
        "demo_corpus = [\n",
        "    \"Climate change is a critical global issue that affects agriculture and health.\",\n",
        "    \"The effects of global warming include rising sea levels and more extreme weather.\",\n",
        "    \"Machine learning improves many real world tasks such as image recognition and language modeling.\",\n",
        "    \"Neural networks can approximate complex functions and are widely used in deep learning.\",\n",
        "    \"The French Revolution began in 1789 and led to major political changes in Europe.\",\n",
        "    \"Photosynthesis is the process by which green plants convert sunlight into energy.\",\n",
        "    \"The mitochondrion is the powerhouse of the cell.\",\n",
        "    \"In 1969, Neil Armstrong became the first person to walk on the Moon.\",\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"SQL stands for Structured Query Language and is used to manage relational databases.\"\n",
        "]\n",
        "\n",
        "student_doc = \"\"\"Global warming causes severe weather and sea level rise, which impacts agriculture.\n",
        "In this essay I discuss how neural nets are used for image recognition tasks.\n",
        "A commonly known fact: The capital of France is Paris.\n",
        "I also talk about photosynthesis as the mechanism plants use to store solar energy.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "os.makedirs('/mnt/data/demo', exist_ok=True)\n",
        "with open('/mnt/data/demo/student1.txt','w',encoding='utf-8') as f:\n",
        "    f.write(student_doc)\n",
        "\n",
        "with open('/mnt/data/demo/corpus.txt','w',encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(demo_corpus))\n",
        "\n",
        "print('Demo files created in /mnt/data/demo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38ca7f0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "7cb4371aef51459cb8946ce1bd3e87a6",
            "2d28ef02473347edb514d4bb405c39e0",
            "663829c04b674d9cbaeddf14f40b8eab",
            "7833c8bf9e7946608d1bbaff87bd8c5c",
            "a37fbffe4aac4891a3de48719ad39a15",
            "ef063a1d98ba48cf885190c3e790eae3",
            "fa14b55be44d4078adff934451375463",
            "c0548e2bb12647cdad8d920f9018158e",
            "e8e173a61ae14ff2a2a85a74afa047e3",
            "603395a607314229967ea7c71e1887a3",
            "2b95fdc4323549b3b0fee86f152fb3c3"
          ]
        },
        "id": "38ca7f0b",
        "outputId": "4966b0be-7e1c-4403-98dd-c51970a2a827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SBERT (this may take a minute)...\n",
            "Encoding corpus...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cb4371aef51459cb8946ce1bd3e87a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index built with 10 sentences\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Load SBERT (bi-encoder) and build FAISS index ---\n",
        "print('Loading SBERT (this may take a minute)...')\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print('Encoding corpus...')\n",
        "corpus_sentences = demo_corpus\n",
        "corpus_embeddings = sbert_model.encode(corpus_sentences, convert_to_numpy=True, show_progress_bar=True)\n",
        "import numpy as np\n",
        "faiss.normalize_L2(corpus_embeddings)\n",
        "d = corpus_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(corpus_embeddings)\n",
        "print('FAISS index built with', index.ntotal, 'sentences')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d64ba5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52d64ba5",
        "outputId": "39b4b694-d958-4fc7-eccb-86c33add0439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Rising sea levels and extreme storms are a result of global warming.\n",
            "Score: 0.891 -> The effects of global warming include rising sea levels and more extreme weather.\n",
            "Score: 0.511 -> Climate change is a critical global issue that affects agriculture and health.\n",
            "Score: 0.197 -> Machine learning improves many real world tasks such as image recognition and language modeling.\n",
            "Score: 0.179 -> Photosynthesis is the process by which green plants convert sunlight into energy.\n",
            "Score: 0.080 -> Neural networks can approximate complex functions and are widely used in deep learning.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Search function and example ---\n",
        "def semantic_search(sentence, top_k=5):\n",
        "    emb = sbert_model.encode([sentence], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(emb)\n",
        "    D, I = index.search(emb, top_k)\n",
        "    results = []\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        if idx < 0: continue\n",
        "        results.append({'sentence': corpus_sentences[idx], 'score': float(score)})\n",
        "    return results\n",
        "\n",
        "query = 'Rising sea levels and extreme storms are a result of global warming.'\n",
        "print('Query:', query)\n",
        "res = semantic_search(query, top_k=5)\n",
        "for r in res:\n",
        "    print(f\"Score: {r['score']:.3f} -> {r['sentence']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f49cad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23f49cad",
        "outputId": "c3893f34-4a3c-4d55-b5de-1f9930f7732e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cross-encoder (this may take another minute)...\n",
            "Reranked results:\n",
            "6.755 -> The effects of global warming include rising sea levels and more extreme weather.\n",
            "-6.128 -> Climate change is a critical global issue that affects agriculture and health.\n",
            "-10.636 -> Photosynthesis is the process by which green plants convert sunlight into energy.\n",
            "-10.706 -> Machine learning improves many real world tasks such as image recognition and language modeling.\n",
            "-10.735 -> Neural networks can approximate complex functions and are widely used in deep learning.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Cross-encoder reranker: rerank top-k candidates for higher precision ---\n",
        "\n",
        "print('Loading cross-encoder (this may take another minute)...')\n",
        "\n",
        "#CrossEncoder from sentence_transformers\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')  # compact reranker\n",
        "\n",
        "def rerank(query, candidates):\n",
        "    #candidates: list of strings\n",
        "    pairs = [[query, c] for c in candidates]\n",
        "    scores = reranker.predict(pairs)  #higher is better\n",
        "    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
        "    return [{'sentence': s, 'rerank_score': float(sc)} for s,sc in ranked]\n",
        "\n",
        "#Example rerank\n",
        "cands = [r['sentence'] for r in res]\n",
        "print('Reranked results:')\n",
        "for item in rerank(query, cands):\n",
        "    print(f\"{item['rerank_score']:.3f} -> {item['sentence']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03bcc9c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03bcc9c2",
        "outputId": "c0bb7897-1377-4d9f-b7f8-a4a20ae67a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student doc sentences:\n",
            "----\n",
            " Global warming causes severe weather and sea level rise, which impacts agriculture.\n",
            "{'num_tokens': 12, 'avg_word_len': 5.833333333333333, 'flesch_reading_ease': 39.55500000000001, 'ttr': 1.0, 'punct_density': 0.14285714285714285, 'noun_ratio': 0.42857142857142855, 'verb_ratio': 0.14285714285714285}\n",
            "----\n",
            " In this essay I discuss how neural nets are used for image recognition tasks.\n",
            "{'num_tokens': 14, 'avg_word_len': 4.5, 'flesch_reading_ease': 65.72500000000001, 'ttr': 1.0, 'punct_density': 0.06666666666666667, 'noun_ratio': 0.3333333333333333, 'verb_ratio': 0.13333333333333333}\n",
            "----\n",
            " A commonly known fact: The capital of France is Paris.\n",
            "{'num_tokens': 10, 'avg_word_len': 4.3, 'flesch_reading_ease': 69.78500000000001, 'ttr': 1.0, 'punct_density': 0.16666666666666666, 'noun_ratio': 0.16666666666666666, 'verb_ratio': 0.08333333333333333}\n",
            "----\n",
            " I also talk about photosynthesis as the mechanism plants use to store solar energy.\n",
            "{'num_tokens': 14, 'avg_word_len': 4.928571428571429, 'flesch_reading_ease': 35.5107142857143, 'ttr': 1.0, 'punct_density': 0.06666666666666667, 'noun_ratio': 0.26666666666666666, 'verb_ratio': 0.2}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def stylometry_features(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    feats = {}\n",
        "    feats['num_tokens'] = len([t for t in doc if t.is_alpha])\n",
        "    feats['avg_word_len'] = sum(len(t.text) for t in doc if t.is_alpha)/max(1,len([t for t in doc if t.is_alpha]))\n",
        "    feats['flesch_reading_ease'] = textstat.flesch_reading_ease(sentence)\n",
        "\n",
        "    #type-token ratio approximation\n",
        "    words = [t.text.lower() for t in doc if t.is_alpha]\n",
        "    feats['ttr'] = len(set(words))/max(1,len(words))\n",
        "    feats['punct_density'] = len([t for t in doc if t.is_punct]) / max(1, len(doc))\n",
        "\n",
        "    #POS ratios (noun, verb)\n",
        "    pos_counts = {}\n",
        "    for t in doc:\n",
        "        pos_counts[t.pos_] = pos_counts.get(t.pos_, 0) + 1\n",
        "    feats['noun_ratio'] = pos_counts.get('NOUN',0)/max(1,len(doc))\n",
        "    feats['verb_ratio'] = pos_counts.get('VERB',0)/max(1,len(doc))\n",
        "    return feats\n",
        "\n",
        "#compute stylometry for sentences in student doc\n",
        "text = open('/mnt/data/demo/student1.txt','r',encoding='utf-8').read()\n",
        "sents = split_sentences(text)\n",
        "print('Student doc sentences:')\n",
        "for s in sents:\n",
        "    print('----\\n', s)\n",
        "    print(stylometry_features(s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "299306c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "299306c2",
        "outputId": "63f10bb5-7b20-4717-a66e-4fb30c23efe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Global warming causes severe weather and sea level rise, which impacts agriculture.\n",
            "{'candidate': 'The effects of global warming include rising sea levels and more extreme weather.', 'semantic': 0.7982355356216431, 'rerank': 3.615565061569214, 'styl_score': 0.6515307692307687, 'fused': 4.875059082782452}\n",
            "{'candidate': 'Climate change is a critical global issue that affects agriculture and health.', 'semantic': 0.71343594789505, 'rerank': 1.6991386413574219, 'styl_score': 0.8589999999999998, 'fused': 4.269998327159882}\n",
            "{'candidate': 'Photosynthesis is the process by which green plants convert sunlight into energy.', 'semantic': 0.24387386441230774, 'rerank': -10.227087020874023, 'styl_score': 1.0, 'fused': 0.4244933784008026}\n",
            "{'candidate': 'Machine learning improves many real world tasks such as image recognition and language modeling.', 'semantic': 0.21470870077610016, 'rerank': -10.82098388671875, 'styl_score': 0.9600285714285712, 'fused': 0.2248280776085172}\n",
            "{'candidate': 'The mitochondrion is the powerhouse of the cell.', 'semantic': 0.07726773619651794, 'rerank': -10.705310821533203, 'styl_score': 0.7777999999999997, 'fused': 0.15884256127357482}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "alpha, beta, gamma = 0.6, 0.3, 0.1\n",
        "\n",
        "def compute_fused_score(query_sent, top_candidates):\n",
        "\n",
        "    #top_candidates: output from semantic_search (list dicts)\n",
        "    candidates = [c['sentence'] for c in top_candidates]\n",
        "    semantic_scores = [c['score'] for c in top_candidates]\n",
        "    rerank_results = rerank(query_sent, candidates)\n",
        "\n",
        "    #map sentence->rerank_score\n",
        "    rerank_map = {r['sentence']: r['rerank_score'] for r in rerank_results}\n",
        "    fused = []\n",
        "\n",
        "    #get min rerank score for normalization safely\n",
        "    rerank_scores_list = [r['rerank_score'] for r in rerank_results] if rerank_results else [0.0]\n",
        "    rer_min = min(rerank_scores_list) if rerank_scores_list else 0.0\n",
        "    for i, cand in enumerate(candidates):\n",
        "        sem = semantic_scores[i]\n",
        "        rer = rerank_map.get(cand, 0.0)\n",
        "\n",
        "        #stylometry: compute difference of flesch_reading_ease as a tiny signal (demo)\n",
        "        styl_q = stylometry_features(query_sent)['flesch_reading_ease']\n",
        "        styl_c = stylometry_features(cand)['flesch_reading_ease']\n",
        "        styl_score = 1.0 - abs((styl_q - styl_c)/50.0)  #normalized rough measure\n",
        "        fused_score = alpha*sem + beta*(rer - rer_min) + gamma*styl_score\n",
        "        fused.append({'candidate': cand, 'semantic': float(sem), 'rerank': float(rer), 'styl_score': float(styl_score), 'fused': float(fused_score)})\n",
        "    merged = sorted(fused, key=lambda x: x['fused'], reverse=True)\n",
        "    return merged\n",
        "\n",
        "\n",
        "query = sents[0]\n",
        "print('Query:', query)\n",
        "topk = semantic_search(query, top_k=5)\n",
        "merged = compute_fused_score(query, topk)\n",
        "for m in merged:\n",
        "    print(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4a6d1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c4a6d1f",
        "outputId": "e063e1d0-9864-4f0e-e01c-fe9fcdb3fba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report saved to /mnt/data/demo/report.json and /mnt/data/demo/report.html\n",
            "{'sentence': 'Global warming causes severe weather and sea level rise, which impacts agriculture.', 'best_match': 'The effects of global warming include rising sea levels and more extreme weather.', 'semantic_score': 0.7982355356216431, 'rerank_score': 3.615565061569214, 'stylometry_score': 0.6515307692307687, 'fused_score': 4.875059082782452, 'stylometry_features': {'num_tokens': 12, 'avg_word_len': 5.833333333333333, 'flesch_reading_ease': 39.55500000000001, 'ttr': 1.0, 'punct_density': 0.14285714285714285, 'noun_ratio': 0.42857142857142855, 'verb_ratio': 0.14285714285714285}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def generate_report(doc_path, out_json='/mnt/data/demo/report.json', out_html='/mnt/data/demo/report.html'):\n",
        "    text = extract_text(doc_path)\n",
        "    sents = split_sentences(text)\n",
        "    report = {'document': str(doc_path), 'sentences': []}\n",
        "    for s in sents:\n",
        "        topk = semantic_search(s, top_k=5)\n",
        "        fused = compute_fused_score(s, topk)\n",
        "        best = fused[0] if fused else {}\n",
        "        entry = {\n",
        "            'sentence': s,\n",
        "            'best_match': best.get('candidate',''),\n",
        "            'semantic_score': best.get('semantic',0.0),\n",
        "            'rerank_score': best.get('rerank',0.0),\n",
        "            'stylometry_score': best.get('styl_score',0.0),\n",
        "            'fused_score': best.get('fused',0.0),\n",
        "            'stylometry_features': stylometry_features(s)\n",
        "        }\n",
        "        report['sentences'].append(entry)\n",
        "\n",
        "    with open(out_json,'w',encoding='utf-8') as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    parts = [f\"<h1>DocInsight Report for {html.escape(str(doc_path))}</h1>\"]\n",
        "    for e in report['sentences']:\n",
        "        parts.append('<div style=\"border:1px solid #ddd;padding:8px;margin:6px;\">')\n",
        "        parts.append(f\"<b>Sentence:</b> {html.escape(e['sentence'])}<br>\")\n",
        "        parts.append(f\"<b>Best match:</b> {html.escape(e['best_match'])} (semantic={e['semantic_score']:.3f}, rerank={e['rerank_score']:.3f}, styl={e['stylometry_score']:.3f}, fused={e['fused_score']:.3f})<br>\")\n",
        "        parts.append(f\"<b>Stylometry features:</b> {html.escape(str(e['stylometry_features']))}<br>\")\n",
        "        parts.append('</div>')\n",
        "    html_text = '\\n'.join(parts)\n",
        "    with open(out_html,'w',encoding='utf-8') as f:\n",
        "        f.write(html_text)\n",
        "    print('Report saved to', out_json, 'and', out_html)\n",
        "    return report\n",
        "\n",
        "report = generate_report('/mnt/data/demo/student1.txt')\n",
        "print(report['sentences'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef6aa53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ef6aa53",
        "outputId": "7cf66006-fd5c-46cb-8fd3-fa988c5f22c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app saved to /mnt/data/demo/app.py (run: streamlit run app.py)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "streamlit_code = r\"\"\"\n",
        "import streamlit as st, json, os\n",
        "from demo_utils import extract_text, split_sentences, generate_report  # demo_utils you'd save from notebook\n",
        "st.title('DocInsight — Mini Demo')\n",
        "uploaded = st.file_uploader('Upload .txt/.pdf/.docx', type=['txt','pdf','docx'])\n",
        "if uploaded:\n",
        "    path = '/tmp/' + uploaded.name\n",
        "    with open(path,'wb') as f:\n",
        "        f.write(uploaded.getbuffer())\n",
        "    report = generate_report(path, out_json='/tmp/report.json', out_html='/tmp/report.html')\n",
        "    st.markdown('### Report (top sentences)')\n",
        "    for s in report['sentences'][:10]:\n",
        "        st.write(s['sentence'])\n",
        "        st.write('**Best match**:', s['best_match'], '| fused:', round(s['fused_score'],3))\n",
        "    st.markdown('Download report files below:')\n",
        "    with open('/tmp/report.html','r',encoding='utf-8') as f:\n",
        "        st.download_button('Download HTML', f.read(), file_name='docinsight_report.html', mime='text/html')\n",
        "\"\"\"\n",
        "\n",
        "with open('/mnt/data/demo/app.py','w',encoding='utf-8') as f:\n",
        "    f.write(streamlit_code)\n",
        "print('Streamlit app saved to /mnt/data/demo/app.py (run: streamlit run app.py)')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "273f742c"
      },
      "source": [
        "# Install ngrok\n",
        "!pip install -q ngrok"
      ],
      "id": "273f742c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9414c281"
      },
      "source": [
        "# Install pyngrok\n",
        "!pip install -q pyngrok"
      ],
      "id": "9414c281",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f01c30a7",
        "outputId": "6f0830ff-b5ce-4d34-aee5-1fffccc54990"
      },
      "source": [
        "# Save utility functions to demo_utils.py\n",
        "utility_code = \"\"\"\n",
        "import os, json, math, tempfile, html, time\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "import faiss\n",
        "import spacy\n",
        "import textstat\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import docx2txt\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(path):\n",
        "    text = []\n",
        "    doc = fitz.open(path)\n",
        "    for page in doc:\n",
        "        text.append(page.get_text())\n",
        "    return \"\\\\n\".join(text)\n",
        "\n",
        "def extract_text(path):\n",
        "    path = str(path)\n",
        "    ext = Path(path).suffix.lower()\n",
        "    if ext == '.pdf':\n",
        "        return extract_text_from_pdf(path)\n",
        "    elif ext == '.docx':\n",
        "        return docx2txt.process(path)\n",
        "    elif ext in ['.txt']:\n",
        "        with open(path,'r',encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    else:\n",
        "        raise ValueError('Unsupported file type: ' + ext)\n",
        "\n",
        "def split_sentences(text):\n",
        "    # basic sentence tokenizer using nltk's punkt\n",
        "    sents = sent_tokenize(text)\n",
        "    # strip and filter short sentences\n",
        "    sents = [s.strip() for s in sents if len(s.strip())>3]\n",
        "    return sents\n",
        "\n",
        "# --- Load SBERT (bi-encoder) and build FAISS index ---\n",
        "# Note: In a real app, you'd load the model and index once outside the function\n",
        "# For this demo, we'll include it here for simplicity, assuming it's pre-loaded or fast enough\n",
        "\n",
        "# Placeholder for pre-loaded model and index (run these cells in the notebook first)\n",
        "# sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# index = faiss.IndexFlatIP(384) # Assuming dimension 384\n",
        "# corpus_sentences = [] # Load your corpus sentences\n",
        "# index.add(sbert_model.encode(corpus_sentences, convert_to_numpy=True))\n",
        "\n",
        "# For the demo, we'll use the pre-loaded objects from the notebook environment\n",
        "# This requires the notebook cells loading sbert_model, index, and corpus_sentences to be run first.\n",
        "global sbert_model, index, corpus_sentences\n",
        "try:\n",
        "    sbert_model\n",
        "    index\n",
        "    corpus_sentences\n",
        "except NameError:\n",
        "    print(\"Warning: SBERT model, FAISS index, or corpus_sentences not found. Please run the relevant cells in the notebook first.\")\n",
        "    # Fallback/placeholder - in a real scenario, handle this properly\n",
        "    sbert_model = None\n",
        "    index = None\n",
        "    corpus_sentences = []\n",
        "\n",
        "\n",
        "def semantic_search(sentence, top_k=5):\n",
        "    if sbert_model is None or index is None or not corpus_sentences:\n",
        "        return [] # Return empty if dependencies not loaded\n",
        "    emb = sbert_model.encode([sentence], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(emb)\n",
        "    D, I = index.search(emb, top_k)\n",
        "    results = []\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        if idx < 0: continue\n",
        "        results.append({'sentence': corpus_sentences[idx], 'score': float(score)})\n",
        "    return results\n",
        "\n",
        "# --- Cross-encoder reranker ---\n",
        "# Placeholder for pre-loaded model\n",
        "# reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "global reranker\n",
        "try:\n",
        "    reranker\n",
        "except NameError:\n",
        "    print(\"Warning: Cross-encoder reranker not found. Please run the relevant cell in the notebook first.\")\n",
        "    reranker = None\n",
        "\n",
        "\n",
        "def rerank(query, candidates):\n",
        "    if reranker is None:\n",
        "        return [{'sentence': s, 'rerank_score': 0.0} for s in candidates] # Return with default score\n",
        "    pairs = [[query, c] for c in candidates]\n",
        "    scores = reranker.predict(pairs)\n",
        "    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
        "    return [{'sentence': s, 'rerank_score': float(sc)} for s,sc in ranked]\n",
        "\n",
        "# --- Stylometry feature extraction ---\n",
        "# Placeholder for pre-loaded model\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "global nlp\n",
        "try:\n",
        "    nlp\n",
        "except NameError:\n",
        "    print(\"Warning: spaCy model not found. Please run the relevant cell in the notebook first.\")\n",
        "    nlp = None\n",
        "\n",
        "\n",
        "def stylometry_features(sentence):\n",
        "    if nlp is None:\n",
        "        return {} # Return empty features if model not loaded\n",
        "    doc = nlp(sentence)\n",
        "    feats = {}\n",
        "    feats['num_tokens'] = len([t for t in doc if t.is_alpha])\n",
        "    feats['avg_word_len'] = sum(len(t.text) for t in doc if t.is_alpha)/max(1,len([t for t in doc if t.is_alpha]))\n",
        "    feats['flesch_reading_ease'] = textstat.flesch_reading_ease(sentence)\n",
        "    # type-token ratio approximation\n",
        "    words = [t.text.lower() for t in doc if t.is_alpha]\n",
        "    feats['ttr'] = len(set(words))/max(1,len(words))\n",
        "    feats['punct_density'] = len([t for t in doc if t.is_punct]) / max(1, len(doc))\n",
        "    # POS ratios (noun, verb)\n",
        "    pos_counts = {}\n",
        "    for t in doc:\n",
        "        pos_counts[t.pos_] = pos_counts.get(t.pos_, 0) + 1\n",
        "    feats['noun_ratio'] = pos_counts.get('NOUN',0)/max(1,len(doc))\n",
        "    feats['verb_ratio'] = pos_counts.get('VERB',0)/max(1,len(doc))\n",
        "    return feats\n",
        "\n",
        "# --- Simple fusion of semantic + reranker + stylometry signals ---\n",
        "alpha, beta, gamma = 0.6, 0.3, 0.1  # weights for semantic, reranker, stylometry (scaled)\n",
        "\n",
        "def compute_fused_score(query_sent, top_candidates):\n",
        "    # top_candidates: output from semantic_search (list dicts)\n",
        "    candidates = [c['sentence'] for c in top_candidates]\n",
        "    semantic_scores = [c['score'] for c in top_candidates]\n",
        "    rerank_results = rerank(query_sent, candidates)\n",
        "    # map sentence->rerank_score\n",
        "    rerank_map = {r['sentence']: r['rerank_score'] for r in rerank_results}\n",
        "    fused = []\n",
        "    # get min rerank score for normalization safely\n",
        "    rerank_scores_list = [r['rerank_score'] for r in rerank_results] if rerank_results else [0.0]\n",
        "    rer_min = min(rerank_scores_list) if rerank_scores_list else 0.0\n",
        "    for i, cand in enumerate(candidates):\n",
        "        sem = semantic_scores[i]\n",
        "        rer = rerank_map.get(cand, 0.0)\n",
        "        # stylometry: compute difference of flesch_reading_ease as a tiny signal (demo)\n",
        "        styl_q = stylometry_features(query_sent).get('flesch_reading_ease', 0.0) # Use get with default\n",
        "        styl_c = stylometry_features(cand).get('flesch_reading_ease', 0.0) # Use get with default\n",
        "        styl_score = 1.0 - abs((styl_q - styl_c)/50.0)  # normalized rough measure\n",
        "        fused_score = alpha*sem + beta*(rer - rer_min) + gamma*styl_score\n",
        "        fused.append({'candidate': cand, 'semantic': float(sem), 'rerank': float(rer), 'styl_score': float(styl_score), 'fused': float(fused_score)})\n",
        "    merged = sorted(fused, key=lambda x: x['fused'], reverse=True)\n",
        "    return merged\n",
        "\n",
        "\n",
        "# --- Report generation ---\n",
        "def generate_report(doc_path, out_json='/tmp/report.json', out_html='/tmp/report.html'):\n",
        "    text = extract_text(doc_path)\n",
        "    sents = split_sentences(text)\n",
        "    report = {'document': str(doc_path), 'sentences': []}\n",
        "    for s in sents:\n",
        "        topk = semantic_search(s, top_k=5)\n",
        "        fused = compute_fused_score(s, topk)\n",
        "        best = fused[0] if fused else {}\n",
        "        entry = {\n",
        "            'sentence': s,\n",
        "            'best_match': best.get('candidate',''),\n",
        "            'semantic_score': best.get('semantic',0.0),\n",
        "            'rerank_score': best.get('rerank',0.0),\n",
        "            'stylometry_score': best.get('styl_score',0.0),\n",
        "            'fused_score': best.get('fused',0.0),\n",
        "            'stylometry_features': stylometry_features(s)\n",
        "        }\n",
        "        report['sentences'].append(entry)\n",
        "    # save JSON\n",
        "    with open(out_json,'w',encoding='utf-8') as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "    # create simple HTML\n",
        "    parts = [f\"<h1>DocInsight Report for {html.escape(str(doc_path))}</h1>\"]\n",
        "    for e in report['sentences']:\n",
        "        parts.append('<div style=\"border:1px solid #ddd;padding:8px;margin:6px;\">')\n",
        "        parts.append(f\"<b>Sentence:</b> {html.escape(e['sentence'])}<br>\")\n",
        "        parts.append(f\"<b>Best match:</b> {html.escape(e['best_match'])} (semantic={e['semantic_score']:.3f}, rerank={e['rerank_score']:.3f}, styl={e['stylometry_score']:.3f}, fused={e['fused_score']:.3f})<br>\")\n",
        "        parts.append(f\"<b>Stylometry features:</b> {html.escape(str(e['stylometry_features']))}<br>\")\n",
        "        parts.append('</div>')\n",
        "    html_text = '\\\\n'.join(parts)\n",
        "    with open(out_html,'w',encoding='utf-8') as f:\n",
        "        f.write(html_text)\n",
        "    print('Report saved to', out_json, 'and', out_html)\n",
        "    return report\n",
        "\"\"\"\n",
        "\n",
        "with open('/mnt/data/demo/demo_utils.py','w',encoding='utf-8') as f:\n",
        "    f.write(utility_code)\n",
        "\n",
        "print('demo_utils.py created in /mnt/data/demo')"
      ],
      "id": "f01c30a7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "demo_utils.py created in /mnt/data/demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab64f5ec",
        "outputId": "b90baca9-8d0d-4e61-ed01-33e6c83d503e"
      },
      "source": [
        "# Run Streamlit with ngrok using pyngrok\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import time\n",
        "\n",
        "time.sleep(2)\n",
        "ngrok.kill()\n",
        "\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "if NGROK_AUTH_TOKEN is None:\n",
        "    print(\"NGROK_AUTH_TOKEN not found in Colab Secrets. Please add it.\")\n",
        "else:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "    print(\"Starting Streamlit app in the background...\")\n",
        "    # Run the Streamlit app in the background\n",
        "    !streamlit run /mnt/data/demo/app.py > /dev/null 2>&1 &\n",
        "\n",
        "    time.sleep(10)\n",
        "\n",
        "    print(\"Attempting to establish ngrok tunnel...\")\n",
        "    try:\n",
        "        public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
        "        print(f\"Streamlit app available at: {public_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error starting ngrok tunnel: {e}\")"
      ],
      "id": "ab64f5ec",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit app in the background...\n",
            "Attempting to establish ngrok tunnel...\n",
            "Streamlit app available at: NgrokTunnel: \"https://734ee730ecaa.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9b0d59",
      "metadata": {
        "id": "3d9b0d59"
      },
      "source": [
        "\n",
        "### Notes & next steps\n",
        "  - Fine-tune SBERT on an academic paraphrase dataset (PAWS/Quora + synthetic adversaries).  \n",
        "  - Train a cross-encoder and stylometry classifier with labeled data.  \n",
        "  - Replace heuristic fusion with a learned logistic regressor trained on validation data.  \n",
        "  - Build a larger corpus and use FAISS indexing strategies (IVF/PQ) for scale.\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7cb4371aef51459cb8946ce1bd3e87a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d28ef02473347edb514d4bb405c39e0",
              "IPY_MODEL_663829c04b674d9cbaeddf14f40b8eab",
              "IPY_MODEL_7833c8bf9e7946608d1bbaff87bd8c5c"
            ],
            "layout": "IPY_MODEL_a37fbffe4aac4891a3de48719ad39a15"
          }
        },
        "2d28ef02473347edb514d4bb405c39e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef063a1d98ba48cf885190c3e790eae3",
            "placeholder": "​",
            "style": "IPY_MODEL_fa14b55be44d4078adff934451375463",
            "value": "Batches: 100%"
          }
        },
        "663829c04b674d9cbaeddf14f40b8eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0548e2bb12647cdad8d920f9018158e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8e173a61ae14ff2a2a85a74afa047e3",
            "value": 1
          }
        },
        "7833c8bf9e7946608d1bbaff87bd8c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_603395a607314229967ea7c71e1887a3",
            "placeholder": "​",
            "style": "IPY_MODEL_2b95fdc4323549b3b0fee86f152fb3c3",
            "value": " 1/1 [00:00&lt;00:00,  8.57it/s]"
          }
        },
        "a37fbffe4aac4891a3de48719ad39a15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef063a1d98ba48cf885190c3e790eae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa14b55be44d4078adff934451375463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0548e2bb12647cdad8d920f9018158e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e173a61ae14ff2a2a85a74afa047e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "603395a607314229967ea7c71e1887a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b95fdc4323549b3b0fee86f152fb3c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}